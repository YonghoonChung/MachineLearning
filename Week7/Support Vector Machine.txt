Large Margin Classification
    Optimization Objective
        Support Vector Machine(SVM)
            - another type of supervised machine learning

        logistic regression
            if y=1, then h_θ(x) ≈ 1 and θ^Tx >> 0
            if y=0, then h_θ(x) ≈ 0 and θ^Tx << 0

            J(θ)=(1/m)*sum(i=1 to m)(-y^(i)log(h_θ(x^(i)))-(1-y(i))log(1-h_θ(x^(i))))

            -log(h_θ(x^(i)) = -log(1/(1+e^(-θ^Tx^(i)))) = z

        hinge loss
            -log(1/(1+e^(-θ^Tx^(i))))
                greater than 1
                for value of x less then 1 -> straight decreasing line
                cost_1(z)
             -log(1-1/(1+e^(-θ^Tx^(i))))
                less than 1
                for value of x greater then 1 -> straight increasing line
                cost_0(z)

        SVM hypothesis
            (min of θ) C*sum(i=1 to m)([y^(i)cost_1(θ^Tx^(i))+(1-y^(i))cost_0(θ^Tx^(i))]) + 1/2 *sum(i=1 to n) θ_j^2

            C = 1/λ
        Hypothesis:
            h_θ(x) = {
                1 if θ^Tx ≥ 0
                0 otherwise
            }

    Large Margin Intuition
        SVM = Large Margin classifier

        If y = 1, θ^Tx ≥ 1 (not just ≥ 0)
        If y = 0, θ^Tx ≤ -1 (not just < 0)

        SVM Decision Boundary
            C=100,000

            Whenever y^(i) = 1:
                θ^Tx^(i) ≥ 1


            Whenever y^(i) = 0:
                θ^Tx^(i) ≤ -1

            min C*0 + 1/2*sum(i=1 to n)θ_j^2

            s.t θ^Tx^(i) ≥ 1 if y^(i)=1
                θ^Tx^(i) ≤ -1 if y^(i)=0

        SVM Decision Boundary: Linearly separable case
            Large margin classifier
            margin = distance of the decision boudary to the nearest example.

        Large margin classifier in presence of outlier
            C very large
                very sensitive to outlier
            C not too large
                not too sensitive to the outlier

    Mathematics Behind Large Margin Classification
        Vector Inner Product
            u=[u1;u2] v=[v1;v2]
            u^Tv = ?
            ||u|| = length of vector u = sqrt(u1^2+u2^2)

            p = length of projection of v onto u
            u^Tv = p * ||u|| = u1v1+u2v2

        SVM Decision Boundary
            min 1/2* sum(j=1 to n)(θ_j^(2)) = 1/2 (θ_1^2+θ_2^2) = 1/2*(sqrt(θ_1^2+θ_2^2))^2
                = 1/2*||θ||^2

             u^Tv = p*||u||


Kernels
    Kernels 1
        Kernels allow us to make complex, non-linear classifiers using SVM

        Non-linear Decision Boundary
            h_θ(x) = {
                1 if θ0+θ1x1 +... ≥0
                0 otherwise 
            } 
            θ0+θ1f1+θ2f2 + ...
            f1 = x1, f2 = x2, f3 = x1x2, f4=x1^2, f5=x2^2

        Given x, compute new feature depending on proximity to landmarks l^(1),l^(2),l^(3)
        f_i=similarity(x,l^(i))=exp(−(∣∣x−l^(i)∣^2)/2σ^2)
                               =exp(−(sum(j=1 to n)(x_j-l_j^(i)^2))/2σ^2)
            similarity() = Gaussian Kernel

            if x ≈ l^(1): (if x and the landmark are close)
                (x_j-l_j^(i)^2) = 0
                f1 ≈ 1 
            if x is far from l^(1): (if x and the landmark are far away from each other)
                (x_j-l_j^(i)^2) = large number
                f1 ≈ 0

            σ is a parameter of the Gaussian Kernel
                it can be modified to increase or decrease the drop-off of feature f_i

        Combined values inside Θ, we can choose these landmarks to get the general shape of decision boundary

    Kernels 2
        way to get landmark is to put them in the exact same location
        -> this give us m landmarks, with one landmark per training example

        f_1=similarity(x,l^(1)),f_2=similarity(x,l^(2)),f_3=similarity(x,l^(3)) ,...

        f_i -> feature vector,f_0=1 to correspond with Θ_0

        Using kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. 
            However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms.
            so kernels are almost always found combined only with SVMs.

        SVM parameters:
            C (=1/λ) {
                Large C : Lower bias, high Variance.
                Small C : higher bias, low Variance
            }
            large σ^2:
                Feature f_i vary more smoothly
                Higher bias, lower Variance

            small σ^2:
                Feature f_i vary less smoothly
                Lower bias, higher Variance

SVMs in Practice
    Using An SVM
        Use SVM software package to solve for parameters θ

        Need to specify:
            Choice of parameter C
            Choice of kernel (similarity function):
                eg. no kernel ("liner kerne")
                    n large, m small
                Gaussian kernel:
                    need to choose σ^2

                    f = exp(−(||x1-x2||^2)/2σ^2)

                    x1 = x^(i)
                    x2 = l^(j)

                    ||x-l||^2 -> v = x - l
                                ||v||^2 = v1^2+v2^2+v3^2+v4^2+...
                                        = (x1-l1)^2 + ....
            Other choice of kernel
                not all similarity functions similarity(x,l) make valid kernels.

                polynomial kernel
                    perform worse
                    k(x,l) = (x^Tl)^2, ... 
                        (x^T*l + constant)^degree
                
                More esoteric:
                    String kernel, chi-square kernel, histogram kernel, intersection kernel

        Multi-class classification
        
        Logisitic regression vs SVMs 
            n = number of feature, m = no. of training examples
            if n is large:
                logistic regression or svm without kernel(linear kernel)
            if n is small:
                SVM with Gaussian kernel
            if n is small, m is large"
                create/add more features, then use logistic regression or SVM without a kernel krne
            
            may take time to learn.