Non-linear Hypothesis
	why we need another algorithm
	when there is too many features, the possibility is too large

	computer vision
		when a computer sees a picture(such as car's handle), reads it as a matrix
		
	50 x 50 pixel images -> 2500 pixels
 		n = 2500
		pixel 1 quadratic features(x_i X x_j) :≈ 3 million features


	so simple registic regression is not a good way to learn complex non-linear hypothesis when n is too large
	
	As the number of our features increase, the number of quadratic or cubic features increase very rapidly and becomes quickly impractical.

Neurons and the Brain
	origins: Algorithm that try to mimic the brain
	Was used in 80s and 90s
	Recent resurgence -> now it is pretty fast

	'one learning algorithm' hypothesis
		Auditory cortex
			auditory cortex learns to see in computer ...?

		Somatosensory cortex
			Somatosensory cortex learns to see in computer

	when you plug in any sense of algorithm, brains learns...
		seeing with your tongue
		Human echolocation(sonar)
		Haptic belt : Direction sense
		Implanting a 3rd eye

Model Representation 1
	Neurons 
	- Dendrite -> input wires
	- Cell body
	- Axon -> output wires

	Neuron model: Logistic unit
		x_0 -> bias unit
	sigmoid(logistic) activation function(using same logistic function as in classification)
		g(z)
		theta parameters are called weights
		[x_0 x_1 x_2] → [    ] → h_θ(x)
`		  layer1          layer2     layer3

	it can have many layers
		first layer -> input layer(x)
		last layer -> output layer(y)
		rest of the layers -> Hidden layer
			a_0^2 , ... , a_n^2 : activation units

	Neural Network
		a_i^j = 'activation' of unit i in layer j
			activation: values computed by and as output by a specific
		Θ^j = matrix of weights controlling function mapping from layer j to layer j+1

		if network has s_j units in layer j, s_(j+1) units in layer j+1, then  Θ^j will be of dimension s_(j+1) x (s_j + 1)

Model Representation 2
	vectorized implementation. 
	a_1^2 = g(z_1^2)
	z_k^(2) = Θ_(k,0)^1x_0 + Θ_(k,1)^1x_1+ ... + Θ_(k,n)^1x_n

	z^(j) = Θ^(j-1)a^(j-1)
		Θ^(j-1) : dimension s_(j) x (n + 1)
		a^(j-1) : vector with (n+1)
		z^(j) : vector with  s_(j)
	a^j = g(z^j)

	Adding a bias unit
		z^(j+1) = Θ^(j) * a^(j)

	Final value
		h_Θ(x) = a^(j+1) = g(z^(j+1))
	just like logistic regression, except that rather than using the original features, is using these new features.

Examples and Intuitions 1
	Non-linear classification example : XOR/XNOR
		x1, x2 are binary(0 or 1)
	
		y = x1 XOR x2
		     x1 XNOR x2
		     NOT (x1 XOR x2)

	Simple example : AND
		x1,x2 ∈ {0,1}
		y = x1 AND x2

		h_Θ(x) = g(-30 +           20x1 +            20x2)
			Θ_10^(1)	        Θ_11^(1)        Θ_12^(1) 
 		
		h_Θ(x) ≈ x1 AND x2

	Example : OR function
		h_Θ(x) = g(-10 + 20x1 + 20x2)
		h_Θ(x) ≈ x1 OR x2	

Examples and Intuitions 2
	NOT x1
		h_Θ(x) = g(10 - 20x1)

	(NOT x1) AND (NOT x2)
		h_Θ(x) = g(10 - 20x1 - 20x2)

	x1 XNOR x2
		it has a hidden layer

Multiclass Classification
	multiple output units : One-vs-all
	
	classify data into multiple classes, hypothesis function returns a vector of value.
	ex) h_Θ(x) = [0;0;1;0] -> third one down or h_Θ(x)_3 which represents the motorcycle.




	