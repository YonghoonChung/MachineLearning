Learning more about Matlab
https://www.coursera.org/learn/machine-learning/supplement/Mlf3e/more-octave-matlab-resources

Multiple Features
	notation:
		n = number of features
		x^i = input(features) of i^th training example.
			looking at the n-th row
		x^(i)_j = value of feature j in i-th training example

	previously: h_θ(x) = θ_0 + θ_1x

	now : 	 h_θ(x) = θ_0 + θ_1x_1+θ_2x_2+θ_3x_3 + ... + θ_nx_n
	-> x = (n+1) X 1
	-> θ = (n+1) X 1

	-> h_θ(x) = θ^Tx : in order to make multiplication


Gradient descent for multiple variables
	hypothesis:
		h_θ(x) = θ^Tx
	parameter:
		θ_0,θ_1,θ_2,...,θ_n
		θ : (n + 1) -dimensional vector
	cost function:
		J(θ) = (1/2m)(i=1 to m)∑((h_θ(x_i)-y_i))^2
	
	gradient descent:
		θ_j := θ_j ​− α((​∂​/∂θ_j)J(θ_0,θ_1,...,θ_n​))
		-> θ_j := θ_j ​− α((​∂​/∂θ_j)J(θ))
	
		previous:
			θ_0 := θ_0 ​− α(1/m(i=1 to m)∑(h_θ(x_i)-y_i))
			θ_1 := θ_1 ​− α(1/m(i=1 to m)∑(h_θ(x_i)-y_i)x_i)	

		new algorithms:
			★θ_j := θ_j ​− α(1/m(i=1 to m)∑(h_θ(x^i)-y^i))x^i_j)★

			θ_0 := θ_0 ​− α(1/m(i=1 to m)∑(h_θ(x_i)-y_i)x^i_0)
			θ_1 := θ_1 ​− α(1/m(i=1 to m)∑(h_θ(x_i)-y_i)x^i_1)
			θ_2 := θ_2 ​− α(1/m(i=1 to m)∑(h_θ(x_i)-y_i)x^i_2)	
			...

		NO BIG DIFFERENCE in PREVIOUS AND NOW

Gradient Descent in Practice 1 - Feature Scaling
	We can speed up g.d. by having each of our input values in roughly the same range.
	θ decends quickly on small range, slowly on large range

	Feature Scaling
		Idea: Make sure features are on a similar scale.

		if the contour plot is ellipse form, it will take along to time to find global mininum
		
		scaling by cumsumer range of values
		0≤x≤1 or -1≤x≤1 (approximately on this range)
		
		θ/(rangeOfθ)

		eg) 
		-100≤x≤100 -> need feature scale
		-0.0001 ≤x≤ 0.0001 -> need feature scale

		-3≤x≤3 -> NO NEED feature scale
		-0.333≤x≤0.333-> NO NEED feature scale

	Mean normalization
		replace x with x - μ to make features have approxiamately zero mean
		(except x_0 = 1)
		μ is usualy range/2
		x = (x - μ)/s
 		μ : avg value of x_1 of training set
		s : range (max - min) or standard deviation

Gradient Descent in Practice 2 - Learning rate (α)
	Debugging  : whether the algorithm is working correctly
	how to choose α

	J(θ) should decrease after every iteration.

	automatic convergence test:	(not really reliable)
		declare convergence if J(θ) decreases by less than 10^-3 in on iteration

	if J(θ) is increasing, g.d. not working
	
	if α is small enough, J(θ) should decrease on every iteration

	∴ α too big -> J(θ)(cost function) is increasing, may not converge
	   α too small -> J(θ) is decreasing slowly, slow convergence

	choose α try ->... ,  0.001, 0.01, 0.1, 1 ,...

Features and Polynomial Regression
	combination
		can combine multiple feature into one
		ex) area = depth * length
	
	polynomial regression
		change the behavoiur of curve -> quadratic, cubic, square root
		-> ! choose feactures and consider feature scaling

	how to fit polynomial : quadratic/ cubic to data
	
	have choice to use feature -> need to decide

Normal Equation
	linear equation -> better way to find optinum value
	
	to find local minima -> J'(θ) = ... = 0
	θ : (n+1) - dimension
	θ = (X^T * X)^-1 * X^T * y
		X : m x (n+1)
		y : m - dimensional vector
	(no need of feature scaling with normal equation)
	
	Gradient Descent vs Normal Equation
		GD : need to choose α, needs many iterations
			works well when n is large
			O(kn^2)

		NE : no need to choose α, don't need to iterate.
			need to compute (X^T * X)^-1
			O(n^3) very slow... 
			slow if n is very large
				n > 10,000

Normal Equation Noninvertilbility (advance concept)
	if (X^T * X) is non-invertible (singular/degenerate)
		-> pinv(X'*X)*X'*y
	
	octave has two inverting functions:
		* pinv -> sudo
			get θ, even if it is non-invertible
		* inv

	Redundant features(linearly dependent)
		-> delete one of redundant features
	Too many features(eg, m≤n)
		-> delete some features or use regularization


Octave Tutorial
	Octave, MATLAB, Python, NumPy, and R.
	Basic operations
		% --> comment
		~= -> !=
		&& -> AND
		|| -> OR
		xor(1,0) 
		
		PS1('>> '); -> changing octave prompt
		; -> supressing output(skip the result)
		  -> in matrix, makes another row
		disp(a) -> print(a) (python)
		disp(sprintf('2 decimals: %0.2f' , a))

		formal long/ format short -> 형변환

		v = 1: 0.1 : 2; making vector starts from 1 to 2 increment with 0.1

		ones(2,3) -> all elements are ones 
		zeros(1,3) -> all elements are zeros

		rand(3,3) -> random number in decimal
		randn(1,3) -> Gaussian random variable
	
		hist(w,a) -> histogram w: matrix a : how closely they are formed

		eye(4) -> 4X4 Identity matrix

	Moving around
		size(A) -> showing the size of matrix A in vector
		length(v) -> showing the longest dimension , good for vector

		loading data
			load featuresX.dat
			load priceY.dat
			load('featuresX.dat')

		who -> variables in workspace
		whos -> showing variable in details

		A(2,:) -> get all values in 2nd row
		A(:,2) -> get all the valus in 2nd col
		A(:,2) = [10;11;12] -> replacing values
		
		A = [A, [100;101;102]];  -> appending another col vec to right

		A(:) -> put all elements of A into a single vector

		C = [A B] -> concatenation -> concat row by row
		C = [A;B] -> concat col by col

	Computing on Data
		* -> dot product

		.* -> element wise multiplication
		.^-> element wise power fuction
		./ -> element wise division
		log(v), exp(v), abs(v)
		
		 -v == -1*v

		A' -> transpose

		max(a) -> col wise max value
		
		a < 3 -> element wise comparison
		find(a<3) -> finding index of true
		
		sum(a),prod(a)
		floor(a), ceil(a)

		max(A, [], 1) -> max of element in row
		max(A, [], 2) -> max of element in col

		pinv(A) -> sudo invers

Plotting Data
	plot(x1,y1)
	+ hold on;
	plot((x2,y2,'r')

	'directory' + print -dpnd 'myPlot.png'
	
	close : figure will get disappear

	subplot(1,2,1) % plot a 1x2 grid

	imagesc(A), colorbar, colormap

	
Control Statements: for,while, if statement
	for loop
		for i = 1:10,
		     v(i) = 2^i;
		end;
	
		for i = indices,
		     disp(i);
		end;
	
	while loop
		i = 1;
		while i <=5,
		     v(i) = 100;
		     i = i+1;
		end;

		while true,
		     v(i) = 999;
		     i = i+ 1;
		     if i == 6,
		          break;
		     end;
		end;
	
	if~ 
		if v(1) == 1,
		     disp('The value is one');
		elseif v(1) ==2,
		     disp(('The value is two');
		else
		     disp('The value is not one or two.');
		end;
	
	function
		function y = squareThisNumber(x)
		y= x^2
			in another file(xxx.m)
			go to the directory and use
			cd 'xxx'
		addpath('directory')


