Density Estimation
    Problem Motivation
        Model -> probability that the example is not anomalous
        ɛ -> dividing lines so we can say which is anomalous

        Anomaly detection example
            Oddly ploted compared to other datas
        Density estimation 
            Model p(x)
            p(x_test) < ɛ -> flag anomaly
            p(x_test) ≥ ɛ -> OK
        Fraud detection:
            find x^(i) -> features of user
            Model p(x)
            p(x_test) < ɛ -> identify unusual user

        if anomalous detector is flagging too many examples, then we need to decrease ɛ

    Gaussian Distribution
        x ~ N(μ,σ^2)
        mean : μ
        variance : σ^2
        SD : σ
        ~ : "distributed as" 
        p(x; μ,σ^2) = exp(-(x-μ)^2/2σ^2)/(sqrt(2π)σ)

        μ = 1/m* sum(1 to m) (x^(i))
        σ^2 = 1/m* sum(1 to m) (x^(i)-μ)^2

    Algorithm
        p(x) = p(x_1;μ_1,σ_1^2)p(x_2;μ_2,σ_2^2)...p(x_n;μ_n,σ_n^2)       
        ∏(j = 1 to n) (p(x_j;μ_j,σ_j^2)) 
        ∏ -> product notation, 1*2*3,...

        choose feature x_1 that which might be indicative of anomalous
        Fit parameters
            μ_j = 1/m* sum(1 to m) (x_j^(i))
            σ_j^2 = 1/m* sum(1 to m) (x_j^(i)-μ_j)^2
        Give new example x, compute p(x)
            p(x) = ∏(j = 1 to n) (p(x_j;μ_j,σ_j^2)) 
                 = ∏(j = 1 to n) (exp(-(x_i-μ_i)^2/2σ_i^2)/(sqrt(2π)σ_i))

Building an Anomaly Detection System
    Developing and Evaluating an Anomaly Detection System
        Making decisioin is much easier if we have a way of evaluating our learning algorithm.(y = 0 if normal, y = 1 if anomalous).

        Fit model p(x) on training set
        on a CV/test example x, predict
            y = {
                1 if p(x) < ɛ (anomaly)
                0 if p(x) ≥ ɛ (normal)
            }
        evalualte metrics
            precision/recall
            f1 score
        
    Anomaly Detection vs. Supervised Learning
        Anomaly detection(use p(x)):
            * very small numver of positive examples (y=1)
            * Large numver of negative example (y=0)
            * Unable to predict future anomalies
            * We have many different "types" of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.
            ex) Fraud detection, Manufacturing(e.g air cratf engines), Monitoring machines in a data center
        Supervised learning:
            * large number of both positive and negative examples
            * enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.
            ex) spam emails, weather prediction, canser classification
    
    Choosing What Features to Use
        if the histogram is skewed, use log(x) to get bell shape
            log(x)
            log(x+1)
            log(x+c)
            x^(1/2)
            x^(1/3)

        Error analysis procedure 
            p(x) is comparable for normal and anomal examples

            Our goal is for p(x) to be large for normal examples and small for anomalous examples.

            figure out new features that will better distinguish the data.

            choose features that might take on unusually large or small values in the event of an anomaly.

Multivariate Gaussian Distribution(MGD)
    MGD
        Instead of modeling p(x_1),p(x_2) separately, we will model p(x) all in one go. 
        Our parameters will be: μ , Σ

        p(x;μ,Σ)= (1/(2π)^(n/2)∣Σ∣^(1/2)(exp(−1/2(x−μ)^TΣ^-1(x−μ)))

    Anomaly Detection using the Multivariate Gaussian Distribution
        1. Fit model p(x) by setting μ , Σ
        2. Given a new example x, compute p(x)
            Flag an anomaly if p(x) < ɛ  

        The original model for p(x) corresponds to a multivariate Gaussian where the contours of p(x;μ,Σ) are axis-aligned.

        Original model vs Multivariate Gaussian
            Original model(use more often):
                p(x_1;μ_1,σ_1^2)p(x_2;μ_2,σ_2^2)...p(x_n;μ_n,σ_n^2)  

                Manually create features to capture anomalies
                Computationally cheaper
                OK even if m is small
            
            Multivariate Gaussian:
                p(x;μ,Σ)= (1/(2π)^(n/2)∣Σ∣^(1/2)(exp(−1/2(x−μ)^TΣ^-1(x−μ)))

                Automatically captures correlations between features
                Computationally more expansive
                Must have m> n or Σ is non-invertible