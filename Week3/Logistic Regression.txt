Classification
	Email: Spam/ Not Spam
	Online: Transaction: Fraudulent(Yes/No)?
	Tumor: Malignant/ Benign

	y ∈ { 0 ,1 } (binary classification)
	   0: "negative class"(benign tumor)
	   1: "positive class" (malignant tumor)

	Linear Regression is not always suitable for binary classificaton

	Classification : y = 0 or 1
		h_θ(x) can be > 1 or < 0

	Logistic Regression :
		0 ≤ h_θ(x) ≤ 1
		
		classification algorithm

Hypothesis Representation
	what is the function to repression classification

	want 0 ≤ h_θ(x) ≤ 1
	
	h_θ(x) = g(θ^Tx)

	g(z) = 1/(1+e^-z)
		Sigmoid function, Logistic function

	∴ h_θ(x) = 1/(1+e^-(θ^Tx))

	h_θ(x) = estimated probability that y =1 on input x

	h_θ(x) = P(y=1|x;θ) = 0.7 "probabiliity that y =1, given x, parameterized by θ"
		P(y=0|x;θ) +P(y=1|x;θ) = 1

Decision boundary
	Decision Boundary is the line that separates the area where y = 0 and where y =1
		h_θ(x) = 0.5

	h_θ(x) = g(θ^Tx)
	g(z) = 1/(1+e^-z)

	"y = 1" if h_θ(x) ≥ 0.5
		g(z) ≥ 0.5 when z ≥ 0
		h_θ(x) = g(θ^Tx) ≥ 0.5 whenever θ^Tx ≥ 0

		∴ θ^Tx ≥ 0
	"y = 0" if h_θ(x) < 0.5
		g(z) < 0.5
		h_θ(x) = g(θ^Tx) < 0.5 -> θ^Tx < 0

		∴ θ^Tx < 0

	y = 1 if θ_0+ θ_1*x_1 + θ_2*x_2 ≥ 0
	y = 0 if θ_0+ θ_1*x_1 + θ_2*x_2 < 0

	sigmoid function g(z) (e.g. θ^T*X) doesn't need to be linear, it can be a circle or any shape of data.

Cost Function
	supervised learning

	how to choose parameter θ
	
	Linear regression : J(θ) = (1/m)(i=1 to m)∑(1/2)((h_θ(x^(i))-y^(i))^2)
	
	Logistic regression : J(θ) = (1/m)(i=1 to m)∑(cost(h_θ(x^(i),y))
	Cost((h_θ(x^(i)),y^(i))) = (1/2)((h_θ(x^(i))-y^(i))^2)
	∴Cost((h_θx , y)) = (1/2)((h_θx - y)^2)
	
	non-convex : not guranteed to converge the mininum value ofJ(θ)
	convex : guranteed to converge the mininum value ofJ(θ)

	Cost((h_θx , y)) =	-log(h_θx) 	if y =1
			-log(1-h_θx) 	if y = 0

Simplified cost function and gradient descent
	simpler cost function
	Cost((h_θx , y)) = -y* log(h_θ(x)) -(1-y)* log(1-h_θ(x))
		IF y=1∶ Cost(h_θ (x),y)= -log⁡(h_θ (x))
		IF y=0∶ Cost(h_θ (x),y)= -log(1- h_θ (x))

	J(θ)	= 1/m ∑_(i=1)^mCost(h_θ (x),y) 
	     	=-1/m[∑_(i=1)^m〖y^i log(h_θ (x^i ))+(1-y^i  )log(1- h_θ (x^i )) 〗]

	Algorithm looks identtical to linear regresion but not equal

Advanced Optimization
	with J(θ) and J'(θ)
	able to have Optimization algoriths:
	---------------------
	1. Gradient descent
	---------------------(for experts)
	2. Conjugate gradient
	3. BFGS
	4. L-BFGS
	
	advantage of 2,3θ,4:
		No need to manually pick learning rate
		Often faster than gradient descent
	disadvantage of 2,3,4:
		more complex
	
	computing to get theta
		function [jVal, gradient] = costFunction(theta)
  			jVal = [...code to compute J(theta)...];
  			gradient = [...code to compute derivative of J(theta)...];
		end

	Using fminunc()
		options = optimset('GradObj', 'on', 'MaxIter', 100);
		initialTheta = zeros(2,1);	
			[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);

Multiclass Classification : one vs all (y={0,1,...n})
	Email foldering/tagging : Work, friends, family, hobby
	Medical diagram : Not ill, cold, flu
	Weather: sunny, cloudy, rain, snow

	with 3 different class
	-> get fake datas and graph that contains one and rest

	h_θ^(i) (x) = P(y=i|x;θ) (i=1,2,3)

	One-vs-all
		Train a LR classifier h_θ^(i) (x) for each class i to predict the probability that y =i
		On a new input x, to make a predicion, pick the class i that maximizes
			max_(i) h_θ^(i) (x)