The Problem of Overfitting
	cause Linear R and Logist R work poorly
	what is overfitting?
		
	poorly fitting -> underfitting, high bias (straight lines)
		when the function is too simple or used too few features
	just right! -> O
	perfectly fitting but fail to generalize-> overfit, high variance

	overfitting : if we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples.
		generalise : how well a hypothesis applies to new examples
		-> usually occurs when there is high order of variables
		
	Addressing overfitting
		when there are many features, it is hard to get overfitting data
		
		options:
		1. Reduce number of features
			Manually select which features to keep
			Use a model selection algorithm
		2. Regularization
			Keep all the features, but reduce magnitude/values of parameters θ_j
			Regularization works well when we have a lot of slight useful features.

Cost Function
	make θ_3, θ_4 really small -> eventually make the quadratic function

	Small values for parameters (Reason why using regularization)
		- simpler hypothesis
		- less prone to overfitting
	
	Regularization cost function : 
		J(θ)=1/2m [(sum)〖1/2 (h_θ x^((i) )- y^((i) ) )^2+λ (sum)θ_j^2 〗]
	Regularization term : λ (sum)θ_j^2 
	Regularizaiton parameter : λ
	
		if λ is set to an extremely large value 
			all the θ will be close to zero, h_θ(x) = θ_0 (straight line) 
			->underfit

Regularized Linear Regression
	J(θ)=1/2m [(sum)〖1/2 (h_θ x^((i) )- y^((i) ) )^2+λ (sum)θ_j^2 〗]
	min(J(θ))
	We can apply to both linear regression and logistic regression.
	
	Repeat {
		θ_0 ≔ θ_0-α (1/m) (sum)〖(h_θ x^((i))-y^((i)) ) x_0^((i)) 〗
		θ_j ≔ θ_j-α [((1/m) (sum)〖(h_θ x^((i) )-y^((i) ) ) x_θ^((i) ) 〗)+λ/m θ_j ]
			j ∈{1,2,...,n}   
	}
	
	(λ/m θ_j) performs regularization

	θ_j≔θ_j (1-α (λ/m))-α (1/m) (sum)〖(h_θ x^((i) )-y^((i) ) ) x_θ^((i) ) 〗
		(1 -α(λ/m)) < 1 eg. 0.99
	
	Normal Equation
		alternate method of the non-iterative normal equation
		L  is a matrix with 0 at the top left and 1's down the diagonal
		(n+1) X (n+1) dimension, identity matrix(not including x_0)

		if m < n, then X^TX is non-invertible. but when we add term (λ∙L)  then it becomes invertible.
		
Regularized Logistic Regression
	previous regularized cost function + regularization term.







	